"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[195],{5239:(e,t,n)=>{n.r(t),n.d(t,{default:()=>b});var a=n(9960),i=n(4996),o=n(2263),l=n(814),r=n(3285),s=n(6010),c=n(7294);const m="features_keug";function u(){return c.createElement("span",{className:(0,s.Z)("stopes")},"stopes")}const d=[{title:"Easy to Use",description:c.createElement(c.Fragment,null,c.createElement(u,null)," was designed to provide a modular API to build and reproduce pipelines core to large translation work. In particular data mining and evaluation. Where you run your pipeline and how you scale it is independent of its core logic. Everything is config-driven so you can easily reproduce and track results."),buttonTxt:"Quickstart",buttonUrl:"docs/quickstart",imageUrl:"img/shovel.svg"},{title:"Batteries Included",description:c.createElement(c.Fragment,null,c.createElement(u,null)," lets you focus on your core data and evaluation needs by providing common modules used for this task and letting you write your pipelines with idiomatic python. Common optimizations have also been built-in to help you scale your work."),buttonTxt:"Learn More",buttonUrl:"docs/stopes",imageUrl:"img/modules.svg"},{title:"State-of-the-art Pipelines",description:c.createElement(c.Fragment,null,c.createElement(u,null)," was developed as part of the Meta AI No Language Left Behind research project. It comes with state-of-the-art pipelines out of the box. You can run our multimodal mining and distillation pipelines and reproduce our research with just a few command lines."),buttonTxt:"E.g. Start Data Mining",buttonUrl:"docs/pipelines/speech_mining",imageUrl:"img/pipelines.svg"}],p=[{title:"No-coding Mining",language:"bash",code:"python -m stopes.pipelines.bitext.global_mining_pipeline \\\n   src_lang=fuv \\\n   tgt_lang=zul \\\n   demo_dir=./demo \\\n   +preset=demo\\\n   output_dir=. \\\n   embed_text=laser3",content:c.createElement("p",null,c.createElement(u,null),"  comes with the Global Mining Pipeline that was used by the NLLB team. You can use it out of the box without extra coding. You will need to setup an environment and create a config file to point to your data, but you can start mining (locally or on a slurm cluster) without any coding. Check out the ",c.createElement(a.Z,{to:"docs/quickstart"},"Quickstart guide"),".")},{title:"Reproducible research",language:"yaml",code:'_target_: stopes.modules.preprocess.train_spm.TrainSpmModule\nconfig:\n  output_dir: ???\n  vocab_size: 50_000\n  input_sentence_size: 5_000_000\n  character_coverage: 0.999995\n  model_type: "unigram"\n  shuffle_input_sentence: True\n  num_threads : 4',content:c.createElement("p",null,c.createElement(u,null)," is based on ",c.createElement(a.Z,{to:"http://hydra.cc/"},"Hydra"),", giving you full control over the behavior of your pipeline. Experiments are easily reproducible along with your results.")},{title:"Modular pipeline definition",language:"python",code:'import asyncio\n\n    import hydra\n    from omegaconf import DictConfig\n    from stopes.core.utils import clone_config\n    from stopes.modules.bitext.indexing.populate_faiss_index import PopulateFAISSIndexModule\n    from stopes.modules.bitext.indexing.train_faiss_index_module import TrainFAISSIndexModule\n\n    # the pipeline\n    async def pipeline(config):\n        # setup a launcher to connect jobs together\n        launcher = hydra.utils.instantiate(config.launcher)\n\n        # train the faiss index\n        trained_index = await launcher.schedule(TrainFAISSIndexModule(\n            config=config.train_index\n        ))\n\n        # pass in the trained index to the next step through config\n        with clone_config(config.populate_index) as config_with_index:\n            config_with_index.index=trained_index\n\n        # fill the index with content\n        populated_index = await launcher.schedule(PopulateFAISSIndexModule(\n            config=config_with_index\n        ))\n        print(f"Indexes are populated in: {populated_index}")\n\n    # setup main with Hydra\n    @hydra.main(config_path="conf", config_name="config")\n    def main(config: DictConfig) -> None:\n        asyncio.run(pipeline(config))\n    ',content:c.createElement(c.Fragment,null,c.createElement("p",null,c.createElement(u,null)," pipelines are composed of modules. No more duplicated, out-of sync code: your most common preprocessing steps can be shared among all your pipelines."),c.createElement("p",null,"You will find in this repository some implementations of a number of modules that are useful for translation data mining and evaluation, Neural Machine Translation data pre-processing and model training. For example, we provide modules to build ",c.createElement(a.Z,{to:"https://faiss.ai/"},"faiss")," indexes, encode text with ",c.createElement(a.Z,{to:"https://github.com/facebookresearch/LASER"},"LASER")," and ",c.createElement(a.Z,{to:"https://huggingface.co/sentence-transformers"},"HuggingFace Transformers"),", mine bitext or train and evaluate ",c.createElement(a.Z,{to:"https://github.com/facebookresearch/fairseq"},"FAIRSEQ")," models."))}];function g(e){let{title:t,description:n,buttonTxt:o,buttonUrl:l,imageUrl:r}=e;const m=(0,i.Z)(r),u=(0,i.Z)(l);return c.createElement("div",{className:(0,s.Z)("col sfeatures")},c.createElement("div",{className:(0,s.Z)("card card--full-height")},m&&c.createElement("div",{className:(0,s.Z)("card__image")},c.createElement("img",{src:m,alt:t,title:t})),c.createElement("div",{className:(0,s.Z)("card__body")},c.createElement("h4",null,t),c.createElement("p",null,n)),o&&l&&c.createElement("div",{className:(0,s.Z)("card__footer")},c.createElement(a.Z,{className:(0,s.Z)("button button--primary button--block"),to:u},o))))}function h(e){let{title:t,children:n,flip:a,language:i}=e;const[o,r]=c.Children.toArray(n),s=c.createElement("div",{class:"col col--4 scontent"},o),m=c.createElement("div",{class:"col"},c.createElement(l.Z,{language:i},r));let u=s,d=m;return a&&(u=m,d=s),c.createElement("div",{className:"ssection"},c.createElement("div",{className:"row"},c.createElement("h3",null,t)),c.createElement("div",{className:"row"},u,d))}function f(){const e=(0,i.Z)("img/banner_bits/nllb.png"),t=(0,i.Z)("img/banner_bits/driving.png"),n=(0,i.Z)("img/banner_bits/stopes.png"),o=(0,i.Z)("img/banner_bits/meta.png"),l=(0,i.Z)("img/logo.svg");return c.createElement("header",{className:(0,s.Z)("sbanner shadow--md")},c.createElement("div",{className:"gh-stars"},c.createElement("iframe",{src:"https://ghbtns.com/github-btn.html?user=facebookresearch&repo=stopes&type=star&count=true&size=large",frameBorder:0,scrolling:0,width:160,height:30,title:"GitHub Stars"})),c.createElement("div",{className:"container"},c.createElement("div",{className:"sblue banner1"},c.createElement("img",{alt:"NO LANGUAGES LEFT BEHIND",src:e})),c.createElement("div",{className:"sblue banner2"},c.createElement("img",{alt:"Driving inclusion through machine translation",src:t})),c.createElement("h1",null,c.createElement("img",{alt:"logo",src:l,className:"logo"}),c.createElement("img",{alt:"stopes",src:n})),c.createElement("div",{className:"banner3"},c.createElement("h2",null,"Large-Scale Translation Tooling"))),c.createElement("div",{className:"bottom"},c.createElement("div",{className:"button-container"},c.createElement(a.Z,{className:(0,s.Z)("button button--secondary button--lg"),to:(0,i.Z)("docs/quickstart")},"Text Mining Quickstart"),c.createElement(a.Z,{className:(0,s.Z)("button button--secondary button--lg"),to:(0,i.Z)("docs/pipelines/speech_mining")},"Multimodal Mining")),c.createElement("div",{className:"banner-meta"},c.createElement("img",{alt:"meta",src:o}))))}function b(){const e=(0,o.Z)(),{siteConfig:t={}}=e;return c.createElement(r.Z,{title:`${t.title}`,description:"Large-scale Translation Tooling"},c.createElement(f,null),c.createElement("main",{className:"container smain"},d&&d.length>0&&c.createElement("section",{className:m},c.createElement("div",null,c.createElement("div",{className:"row"},d.map((e=>{let{title:t,imageUrl:n,description:a,buttonTxt:i,buttonUrl:o}=e;return c.createElement(g,{key:t,title:t,imageUrl:n,description:a,buttonTxt:i,buttonUrl:o})}))))),c.createElement("section",null,p.map(((e,t)=>{let{title:n,language:a,code:i,content:o}=e;return c.createElement(h,{key:n,flip:t%2,language:a,title:n},o,i)})))))}}}]);
"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[924],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>u});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=p(n),u=i,h=m["".concat(l,".").concat(u)]||m[u]||c[u]||o;return n?a.createElement(h,r(r({ref:t},d),{},{components:n})):a.createElement(h,r({ref:t},d))}));function u(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var p=2;p<o;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},2024:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(7462),i=(n(7294),n(3905));const o={sidebar_position:1},r="Speech Mining Pipeline",s={unversionedId:"pipelines/speech_mining",id:"pipelines/speech_mining",title:"Speech Mining Pipeline",description:"With the Seamless Communication project, FAIR has introduced a new mechanism for speech mining. In the stopesV1, you could mine large text datasets to create aligned text accross languages. This was useful to train machine translation algorithms. From stopesV2 onwards, we introduce a mechanism that lets you mine speech and text together accross languages to create aligned multimodal datasets for training and evaluating speech tasks. This mining is based on the SONAR multimodal/multilingual embedding space.",source:"@site/docs/pipelines/speech_mining.md",sourceDirName:"pipelines",slug:"/pipelines/speech_mining",permalink:"/stopes/docs/pipelines/speech_mining",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/pipelines/speech_mining.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"quickstartSidebar",previous:{title:"Expressive parallel alignments",permalink:"/stopes/docs/pipelines/expressive_alignments"},next:{title:"Global Mining Pipeline",permalink:"/stopes/docs/pipelines/global_mining"}},l={},p=[{value:"Installation",id:"installation",level:2},{value:"Preset configuration",id:"preset-configuration",level:2},{value:"Run the mining pipeline",id:"run-the-mining-pipeline",level:2},{value:"Audio data format and segmentation",id:"audio-data-format-and-segmentation",level:2}],d={toc:p};function c(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"speech-mining-pipeline"},"Speech Mining Pipeline"),(0,i.kt)("p",null,"With the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/seamless_communication"},"Seamless Communication project"),", FAIR has introduced a new mechanism for speech mining. In the ",(0,i.kt)("inlineCode",{parentName:"p"},"stopesV1"),", you could mine large text datasets to create aligned text accross languages. This was useful to train machine translation algorithms. From ",(0,i.kt)("inlineCode",{parentName:"p"},"stopesV2")," onwards, we introduce a mechanism that lets you mine speech and text together accross languages to create aligned multimodal datasets for training and evaluating speech tasks. This mining is based on the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/SONAR"},"SONAR multimodal/multilingual embedding space"),"."),(0,i.kt)("h2",{id:"installation"},"Installation"),(0,i.kt)("p",null,"Speech mining requires the installation of ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/fairseq2"},"fairseq2")," and ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/SONAR"},"SONAR"),". You can install these with:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"pip install 'stopes[speech,mining,sonar_mining]'\n")),(0,i.kt)("p",null,"or if you are installing a local checkout of this repository:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"pip install '.[speech,mining,sonar_mining]'\n")),(0,i.kt)("p",null,"The above message should install SONAR and fairseq2 with their default settings. If you want to install fairseq2 with custom support for your hardware (i.e. GPU suppport with different CUDA versions), see the installation instructions in the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/fairseq2"},"fairseq2")," documentation to build the package for your own machine."),(0,i.kt)("h2",{id:"preset-configuration"},"Preset configuration"),(0,i.kt)("p",null,"The speech mining pipeline is an extension of the ",(0,i.kt)("a",{parentName:"p",href:"https://facebookresearch.github.io/stopes/docs/pipelines/global_mining"},"global mining")," pipeline. We recommend reading that page first to understand the base configuration for text to text mining."),(0,i.kt)("p",null,"While in the global mining, you run one encoder for all laguages, in speech mining, you can configure different encoders of different modalities (text or speech), depending on whether you want to mine text-text, speech-text, text-speech or speech-speech with the SONAR space encoders. You can do this within a language (e.g. to mine aligned text-speech in English), or accross languages (e.g. to mine aligned speech-speech between Catalan and Korean)."),(0,i.kt)("p",null,"More specifically, in speech mining, we use a preset configuration to set up the embedding modules for each language under the ",(0,i.kt)("inlineCode",{parentName:"p"},"lang_configs"),". Note that in your configuration, ",(0,i.kt)("inlineCode",{parentName:"p"},"lang_configs")," might end up having multiple entries for the same language, but with different modalities."),(0,i.kt)("p",null,"Below is an example if you want to mine text-speech within French (i.e. aligning between French audios and text). You can just replace the ",(0,i.kt)("inlineCode",{parentName:"p"},"lang_configs")," section in ",(0,i.kt)("inlineCode",{parentName:"p"},"stopes/pipelines/bitext/conf/preset/demo.yaml")," with the following setting:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'lang_configs:\n  frA:\n    data:\n      data_version: 23H1RCLSP\n      iteration: 1\n      data_shard_dir: /path/tothe/rawaudio/dataset\n      shard_type: speech\n      bname: speech\n      shard_list: null\n      shard_glob: ${.data_shard_dir}/speech/*.ogg\n      nl_file_template: "{lang}.nl"\n\n    embed_speech:\n      preprocess: null\n      encoder:\n        _target_: stopes.modules.preprocess.mining_speech_encoder.Sonar2MiningSpeechEncoder\n        encoder_model: NAME_OF_SONAR_ENCODER_MODEL\n        _name: sonar2_speech_encoder\n        spm_model: null # unused\n        spm_vocab: null # unused\n        mini_batch_size: null\n        fp16: true\n        gpu: true\n        num_processes: 4\n  fr:\n    data:\n      data_version: 23H1RCL\n      iteration: 1\n      data_shard_dir: /path/tothe/rawtext/dataset\n      shard_type: text\n      bname: text\n      shard_list: null\n      shard_glob: ${.data_shard_dir}/text/text.txt\n      meta_glob: ${.data_shard_dir}/text/meta.txt\n      nl_file_template: "{lang}.nl"\n\n    embed_text:\n      encoder:\n        _target_: stopes.modules.preprocess.sonar_sentence_encoder.SonarTextEncoder\n        _name: NAME_OF_SONAR_ENCODER_MODEL\n        spm_model: null # unused\n        spm_vocab: null # unused\n')),(0,i.kt)("p",null,"In this sample config, we have set a text data source ",(0,i.kt)("inlineCode",{parentName:"p"},"fr")," and an audio data source ",(0,i.kt)("inlineCode",{parentName:"p"},"frA"),'. The example assumes they are the same language, but they could be different languages, or they could both be audio or text. The audio data source uses a SONAR speech encoder while the text source uses the text encoder. Make sure to refer to the SONAR model cards to choose the appropriate encoder model to specify in each config entry. Replace "NAME_OF_SONAR_ENCODER_MODEL" in the config above with the name of the SONAR model card (You can find the list of available model card in the ',(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/SONAR/tree/main/sonar/cards"},"SONAR Github repository"),")."),(0,i.kt)("h2",{id:"run-the-mining-pipeline"},"Run the mining pipeline"),(0,i.kt)("p",null,"The above setting is analog to the ",(0,i.kt)("inlineCode",{parentName:"p"},"embed_text")," step in the ",(0,i.kt)("a",{parentName:"p",href:"https://facebookresearch.github.io/stopes/docs/pipelines/global_mining"},"global mining"),". The different here is that we can use either ",(0,i.kt)("inlineCode",{parentName:"p"},"embed_speech")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"embed_text")," to encode the data of different modalities. The rest of the mining pipeline is similar to the one described on the global mining page."),(0,i.kt)("p",null,"We provide the example preset in ",(0,i.kt)("inlineCode",{parentName:"p"},"stopes/pipelines/bitext/conf/preset/demo_speechmine.yaml"),", so you can simply run:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python -m stopes.pipelines.bitext.global_mining_pipeline src_lang=frA tgt_lang=fr demo_text_dir=.../stopes-repo/demo demo_audio_dir=[YOUR AUDIO DIR] +preset=demo_speechmine output_dir=.\n")),(0,i.kt)("h2",{id:"audio-data-format-and-segmentation"},"Audio data format and segmentation"),(0,i.kt)("p",null,"In order to run the above example, you would need to provide your own audio data. Note that the audio dataset shards are text files containing segmentation information about your raw audio. This follows the format:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"audio_file_name start_timestamp end_timestamp batch_no\n")),(0,i.kt)("p",null,"where ",(0,i.kt)("inlineCode",{parentName:"p"},"start/end")," timestamps are timestamps of a speech segment within the audio file. ",(0,i.kt)("inlineCode",{parentName:"p"},"batch_no")," is a batching number for this segment. You can use it to batch segments of similar length together for faster embedding, or just leave it set to 0."),(0,i.kt)("p",null,"This means that you need to run the speech segmentation separately. There are many ways to segment audio, one of them is to use our internal VADSegmentation, found in ",(0,i.kt)("inlineCode",{parentName:"p"},"stopes.modules.speech.vad_segment_audio.VADSegmentAudioModule"),". Below is the example of the updated ",(0,i.kt)("inlineCode",{parentName:"p"},"lang_configs")," for the audio data with segmentation option:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'lang_configs:\n  frA:\n    data:\n      data_version: 23H1RCLSP\n      iteration: 1\n      data_shard_dir: /path/tothe/segmented_dataset\n      shard_type: speech\n      bname: speech\n      shard_list: null\n      shard_glob: ${.data_shard_dir}/speech/*.ogg\n      nl_file_template: "{lang}.nl"\n\n    segment_audio:\n      _target_: stopes.modules.speech.vad_segment_audio.VADSegmentAudioModule\n      lang: fr\n      shards: /path/tothe/rawaudio/dataset\n      max_duration_in_seconds: null\n      output_dir: /path/tothe/segmented_dataset\n      model: [MODEL_TO_SILERO_VAD]\n      hard_limit_min_length: 1.0\n\n    embed_speech:\n      preprocess: null\n      encoder:\n        _target_: stopes.modules.preprocess.mining_speech_encoder.Sonar2MiningSpeechEncoder\n        encoder_model: NAME_OF_SONAR_ENCODER_MODEL\n        _name: sonar2_speech_encoder\n        spm_model: null # unused\n        spm_vocab: null # unused\n        mini_batch_size: null\n        fp16: true\n        gpu: true\n        num_processes: 4\n')),(0,i.kt)("p",null,"Where the ",(0,i.kt)("inlineCode",{parentName:"p"},"MODEL_TO_SILERO_VAD")," point to the silero VAD checkpoint found in e.g. ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/snakers4/silero-models"},"Silero models hub"),"."))}c.isMDXComponent=!0}}]);
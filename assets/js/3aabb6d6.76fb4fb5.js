"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[158],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=p(n),h=r,m=d["".concat(s,".").concat(h)]||d[h]||u[h]||i;return n?a.createElement(m,o(o({ref:t},c),{},{components:n})):a.createElement(m,o({ref:t},c))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},6834:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const i={},o="BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric",l={unversionedId:"eval/blaser",id:"eval/blaser",title:"BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric",description:"BLASER leverages a multilingual multimodal encoder to directly encode the speech segments for source input, translation output and reference into a shared embedding space and computes a score of the translation quality that can be used as a proxy to human evaluation.",source:"@site/docs/eval/blaser.md",sourceDirName:"eval",slug:"/eval/blaser",permalink:"/stopes/docs/eval/blaser",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/eval/blaser.md",tags:[],version:"current",frontMatter:{},sidebar:"quickstartSidebar",previous:{title:"ALTI+",permalink:"/stopes/docs/eval/alti"}},s={},p=[{value:"Files",id:"files",level:2},{value:"Install",id:"install",level:2},{value:"Using the pipeline",id:"using-the-pipeline",level:2},{value:"Citation",id:"citation",level:2},{value:"License",id:"license",level:2}],c={toc:p};function u(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"blaser-a-text-free-speech-to-speech-translation-evaluation-metric"},"BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric"),(0,r.kt)("p",null,"BLASER leverages a multilingual multimodal encoder to directly encode the speech segments for source input, translation output and reference into a shared embedding space and computes a score of the translation quality that can be used as a proxy to human evaluation."),(0,r.kt)("p",null,"In this folder you can find tools to use BLASER to score speech translation and to train the BLASER supervised model. You can also download a ",(0,r.kt)("a",{parentName:"p",href:"http://dl.fbaipublicfiles.com/blaser/blaser.tar.gz"},"pre-trained model"),"."),(0,r.kt)("p",null,"BLASER relies on ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/fairseq/blob/ust/examples/speech_matrix/speech_laser_encoders.md"},"SpeechLASER embeddings"),", follow the instructions there to download the embeddings and embed your speech segments."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"[!NOTE]","\nBlaserV2.0 has now been released and lives in the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/SONAR#predicting-sentence-similarity-with-blaser-20-models"},"SONAR repository"),". We recommend using this new model and embedding space for your future uses of BLASER evalution")),(0,r.kt)("h2",{id:"files"},"Files"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"model.py")," contains the BLASER supervised model definition."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"train.py")," contains a script that you can use to train the BLASER model."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"score.py")," contains a script that you can use to score speech segments translations.")),(0,r.kt)("p",null,"The train and score scripts are configured using ",(0,r.kt)("a",{parentName:"p",href:"https://hydra.cc/"},"hydra"),", you can look at the base configurations in ",(0,r.kt)("inlineCode",{parentName:"p"},"blaser/conf/score.yaml")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"blaser/conf/train.yaml"),", you will have to specify the ",(0,r.kt)("inlineCode",{parentName:"p"},"???")," fields, either in your own configs, or over the CLI. Both scripts take pre-embedded files."),(0,r.kt)("h2",{id:"install"},"Install"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"git clone https://github.com/facebookresearch/stopes.git\ncd stopes\npip install -e '.[blaser]'\n")),(0,r.kt)("h2",{id:"using-the-pipeline"},"Using the pipeline"),(0,r.kt)("p",null,"Blaser requires embedded speech segments to compute the evaluation metric. A good way to get these embeddings is to use ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/fairseq/blob/ust/examples/speech_matrix/speech_laser_encoders.md"},"SpeechLASER embeddings"),"."),(0,r.kt)("p",null,"We provide a pipeline that will compute the embeddings and feed them to the blaser model for you."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"}," Model ")),(0,r.kt)("p",null,"You can download the model from ",(0,r.kt)("a",{parentName:"p",href:"https://dl.fbaipublicfiles.com/blaser/blaser.tar.gz"},"here"),"."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"}," Requirements ")),(0,r.kt)("p",null,"To be able to use this pipeline, you will need the version of fairseq that has the Wav2VevLaser model implementation, this is currently in the ",(0,r.kt)("inlineCode",{parentName:"p"},"ust")," branch of fairseq. Make sure to clone the repository, switch to that branch and install fairseq in your environment with: ",(0,r.kt)("inlineCode",{parentName:"p"},"pip install -e ."),". It is recommended to do this before install stopes as per above."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"}," Run the pipeline ")),(0,r.kt)("p",null,"You will need to pass three sets of speech segments to get a blaser score:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"the source audio (",(0,r.kt)("inlineCode",{parentName:"li"},"src"),")"),(0,r.kt)("li",{parentName:"ul"},"the translated audio (",(0,r.kt)("inlineCode",{parentName:"li"},"target"),")"),(0,r.kt)("li",{parentName:"ul"},"the reference audio (",(0,r.kt)("inlineCode",{parentName:"li"},"ref"),")")),(0,r.kt)("p",null,"The set of speech segments have to be organised in a tsv manifest pointing to the audio files. The format for each input audio data tsv file is:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"<root_dir>\n<audio_path>\\t<num_frames>\n<audio_path>\\t<num_frames>\n...\n...\n")),(0,r.kt)("p",null,"Rows in each manifest TSV files should align (line 1 of the src manifest should be translated on line 1 of the tgt manifest)."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"root_dir")," is on the first line of the TSV, it's a path to where all the following files can be found"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"audio_path")," can be either:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"a filename in ",(0,r.kt)("inlineCode",{parentName:"li"},"root_dir")," for a .npy/.wav/.flac/.ogg file"),(0,r.kt)("li",{parentName:"ul"},"a filename of stored ZIP file, in ",(0,r.kt)("inlineCode",{parentName:"li"},"root_dir"),', with slicing info: "',"[zip_path]",":","[offset]",":","[length]",'" to find the bytes in the uncompressed zip archive containing that particular file')))),(0,r.kt)("p",null,"Check out the ",(0,r.kt)("inlineCode",{parentName:"p"},"demo/iwslt_blaser_eval/mk_manifest.py")," script to see how to generate such a manifest file."),(0,r.kt)("p",null,"You can then run the pipeline with:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"python -m stopes.pipelines.eval.eval_blaser output_dir=YOUROUTPUTDIRECTORY src_manifest=PATHTOSOURCEMANIFEST.tsv tgt_manifest=PATHTOTARGETMANIFEST.tsv ref_manifest=PATHTOREFERENCEMANIFEST.tsv src_lang=en tgt_lang=de\n")),(0,r.kt)("p",null,"where ",(0,r.kt)("inlineCode",{parentName:"p"},"src_lang")," is the language of your source audio and tgt_lang is the target language. This is used to lookup the correct encoder model as specified by ",(0,r.kt)("inlineCode",{parentName:"p"},"stopes/pipelines/eval/conf/eval_blaser.yaml"),". You can download pre-trained encoders from the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/fairseq/blob/ust/examples/speech_matrix/speech_laser_encoders.md"},"SpeechMatrix project"),". By default, the encoder used for the reference is the same as the target one, you can override this with ",(0,r.kt)("inlineCode",{parentName:"p"},"ref_lang=..")," in the command arguments."),(0,r.kt)("h2",{id:"citation"},"Citation"),(0,r.kt)("p",null,"If you use ",(0,r.kt)("inlineCode",{parentName:"p"},"blaser")," in your work or any of its models, please cite:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bibtex"},"@misc{blaser2022,\n  title={BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric},\n  author={Mingda Chen and Paul-Ambroise Duquenne and Pierre Andrews and Justine Kao and Alexandre Mourachko and Holger Schwenk and Marta R. Costa-juss\xe0},\n  year={2022}\n  doi = {10.48550/ARXIV.2212.08486},\n  url = {https://arxiv.org/abs/2212.08486},\n  publisher = {arXiv},\n}\n")),(0,r.kt)("h2",{id:"license"},"License"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"blaser")," is MIT licensed, as found in the LICENSE file in the root directory."))}u.isMDXComponent=!0}}]);
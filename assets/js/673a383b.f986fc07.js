"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[924],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=p(n),h=i,m=d["".concat(l,".").concat(h)]||d[h]||u[h]||o;return n?a.createElement(m,r(r({ref:t},c),{},{components:n})):a.createElement(m,r({ref:t},c))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var p=2;p<o;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},2024:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(7462),i=(n(7294),n(3905));const o={sidebar_position:1},r="Speech Mining Pipeline",s={unversionedId:"pipelines/speech_mining",id:"pipelines/speech_mining",title:"Speech Mining Pipeline",description:"With the Seamless Communication project, FAIR has introduced a new mechanism for speech mining. In the stopesV1, you could mine large text datasets to create aligned text accross languages. This was useful to train machine translation algorithms. From stopesV2, we introduce a mecanism that lets you mine speech and text together accross languages to create aligned multimodal datasets for training and evaluating speech tasks. This mining is based on the SONAR multimodal/multilingual embedding space.",source:"@site/docs/pipelines/speech_mining.md",sourceDirName:"pipelines",slug:"/pipelines/speech_mining",permalink:"/stopes/docs/pipelines/speech_mining",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/pipelines/speech_mining.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"quickstartSidebar",previous:{title:"Prebuilt Pipelines",permalink:"/stopes/docs/category/prebuilt-pipelines"},next:{title:"Global Mining Pipeline",permalink:"/stopes/docs/pipelines/global_mining"}},l={},p=[{value:"Installation",id:"installation",level:2},{value:"Configuration",id:"configuration",level:2}],c={toc:p};function u(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"speech-mining-pipeline"},"Speech Mining Pipeline"),(0,i.kt)("p",null,"With the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/seamless_communication"},"Seamless Communication project"),", FAIR has introduced a new mechanism for speech mining. In the ",(0,i.kt)("inlineCode",{parentName:"p"},"stopesV1"),", you could mine large text datasets to create aligned text accross languages. This was useful to train machine translation algorithms. From ",(0,i.kt)("inlineCode",{parentName:"p"},"stopesV2"),", we introduce a mecanism that lets you mine speech and text together accross languages to create aligned multimodal datasets for training and evaluating speech tasks. This mining is based on the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/SONAR"},"SONAR multimodal/multilingual embedding space"),"."),(0,i.kt)("h2",{id:"installation"},"Installation"),(0,i.kt)("p",null,"Speech mining requires the installation of ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/fairseq2"},"fairseq2")," and ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/SONAR"},"SONAR"),". You can install these with:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"pip install 'stopes[speech,mining,speech_mining]'\n")),(0,i.kt)("p",null,"or if you are installing a local checkout of this repository:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"pip install '.[speech,mining,speech_mining]'\n")),(0,i.kt)("p",null,"If fairseq2 does not provide a build for your hardware, see the installation instructions in the fairseq2 documentation to build the package for your own machine."),(0,i.kt)("h2",{id:"configuration"},"Configuration"),(0,i.kt)("p",null,"The speech mining pipeline is an extension of the ",(0,i.kt)("a",{parentName:"p",href:"https://facebookresearch.github.io/stopes/docs/pipelines/global_mining"},"global mining")," pipeline. We recommend reading that page first to understand the base configuration for text to text mining."),(0,i.kt)("p",null,"You can decide to mine text-text, speech-text, text-speech or speech-speech with the SONAR space encoders, you can do this withing language (e.g. to mine aligned text-speech in English), or accross languages (e.g. to mine aligned speech-speech between Catalan and Korean). To do this, you need to configure the type of encoder that you want to use for the data you are feeding the global mining pipeline. You will want to set different language configurations in your mining ",(0,i.kt)("inlineCode",{parentName:"p"},"yaml")," preset. While the configurations are called language, you might end up creating multiple entries for the same language, but with different modalities."),(0,i.kt)("p",null,"For example, if you wanted to mine text-speech within French, you will add something like:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'lang_configs:\n  frA:\n    data:\n      data_version: 23H1RCLSP\n      iteration: 1\n      data_shard_dir: /path/tothe/rawaudio/dataset\n      shard_type: speech\n      bname: speech\n      shard_list: null\n      shard_glob: ${.data_shard_dir}/speech/*.ogg\n      nl_file_template: "{lang}.nl"\n\n    embed_speech:\n      preprocess: null\n      encoder:\n        _target_: stopes.modules.preprocess.mining_speech_encoder.Sonar2MiningSpeechEncoder\n        encoder_model: NAME_OF_SONAR_ENCODER_MODEL\n        _name: sonar2_speech_encoder\n        spm_model: null # unused\n        spm_vocab: null # unused\n        mini_batch_size: null\n        fp16: true\n        gpu: true\n        num_processes: 4\n  fr:\n    data:\n      data_version: 23H1RCL\n      iteration: 1\n      data_shard_dir: /path/tothe/rawtext/dataset\n      shard_type: text\n      bname: text\n      shard_list: null\n      shard_glob: ${.data_shard_dir}/text/text.txt\n      meta_glob: ${.data_shard_dir}/text/meta.txt\n      nl_file_template: "{lang}.nl"\n\n    embed_text:\n      encoder:\n        _target_: stopes.modules.preprocess.sonar_sentence_encoder.SonarTextEncoder\n        _name: NAME_OF_SONAR_ENCODER_MODEL\n        spm_model: null # unused\n        spm_vocab: null # unused\n')),(0,i.kt)("p",null,"Note that the audio dataset shards are text files containing segmentation information about your raw audio. This follows the format:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"audio_file_name start_timestamp end_timestamp batch_no\n")),(0,i.kt)("p",null,"where ",(0,i.kt)("inlineCode",{parentName:"p"},"start/end")," timestamps are timestamps of a speech segment within the audio file. ",(0,i.kt)("inlineCode",{parentName:"p"},"batch_no")," is a batching number for this segment. You can use it to batch segments of similar length together for faster embedding, or just leave it set to 0. "),(0,i.kt)("p",null,"This means that you need to run the speech segmentation separately. There are many ways to segment audio, so we do not discuss this here."),(0,i.kt)("p",null,"In this sample config, we have set a text data source ",(0,i.kt)("inlineCode",{parentName:"p"},"fr")," and an audio data source ",(0,i.kt)("inlineCode",{parentName:"p"},"frA"),". The example assumes they are the same language, but they could be different languages, or they could both be audio or text. The audio data source uses a SONAR speech encoder while the text source uses the text encoder. Make sure to refer to the SONAR model cards to choose the appropriate encoder model to specify in each config entry. You will need to set ",(0,i.kt)("inlineCode",{parentName:"p"},"encoder_model")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"_name")," to the name of the SONAR model card. fairseq2 will then download the model for you if needed."),(0,i.kt)("p",null,"The rest of the mining pipeline is similar to the one described on the global mining page, please see the description there for run examples."))}u.isMDXComponent=!0}}]);
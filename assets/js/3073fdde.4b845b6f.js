"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[840],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=p(n),m=i,h=c["".concat(s,".").concat(m)]||c[m]||u[m]||o;return n?a.createElement(h,r(r({ref:t},d),{},{components:n})):a.createElement(h,r({ref:t},d))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,r[1]=l;for(var p=2;p<o;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},8716:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var a=n(7462),i=(n(7294),n(3905));const o={sidebar_position:1},r="Global Mining Pipeline",l={unversionedId:"pipelines/global_mining",id:"pipelines/global_mining",title:"Global Mining Pipeline",description:"You can launch the mining for a pair of languages with the following command:",source:"@site/docs/pipelines/global_mining.md",sourceDirName:"pipelines",slug:"/pipelines/global_mining",permalink:"/stopes/docs/pipelines/global_mining",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/pipelines/global_mining.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"quickstartSidebar",previous:{title:"Prebuilt Pipelines",permalink:"/stopes/docs/category/prebuilt-pipelines"},next:{title:"NLLB Monolingual Pipeline",permalink:"/stopes/docs/pipelines/monolingual"}},s={},p=[{value:"Outputs and Working Dir",id:"outputs-and-working-dir",level:2},{value:"Data",id:"data",level:2},{value:"Outputs",id:"outputs",level:2},{value:"Evaluation data",id:"evaluation-data",level:2},{value:"Example overrides",id:"example-overrides",level:2}],d={toc:p};function u(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"global-mining-pipeline"},"Global Mining Pipeline"),(0,i.kt)("h1",{id:"basic-usage"},"Basic Usage"),(0,i.kt)("p",null,"You can launch the mining for a pair of languages with the following command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python -m stopes.pipelines.bitext.global_mining_pipeline src_lang=fuv tgt_lang=zul demo_dir=.../stopes-repo/demo +preset=demo output_dir=. embed_text=laser2\n")),(0,i.kt)("p",null,"(see the demo doc for a quick understanding of the ",(0,i.kt)("inlineCode",{parentName:"p"},"+preset")," override)"),(0,i.kt)("p",null,"This will run the required steps and try to re-use whatever step outputs has already been computed. So if you run this exact command multiple times (e.g. after a pre-emption in slurm), it will start from where it failed instead of recomputing everything."),(0,i.kt)("p",null,"Here is an example log:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"[global_mining][INFO] - output: ....../mining/global_mining/outputs/2021-11-02/08-56-40\n[global_mining][INFO] - working dir: ....../mining/global_mining/outputs/2021-11-02/08-56-40\n[mining_utils][WARNING] - No mapping for lang bn\n[embed_text][INFO] - Number of shards: 55\n[embed_text][INFO] - Embed bn (hi), 55 files\n[stopes_launcher][INFO] - for encode.bn.55 found 55 already cached array results,0 left to compute out of 55\n[train_faiss_index][INFO] - lang=bn, sents=135573728, required=40000000, index type=OPQ64,IVF65536,PQ64\n[stopes_launcher][INFO] - index-train.bn.iteration_2 done from cache\n[stopes_launcher][INFO] - for populate_index.OPQ64,IVF65536,PQ64.bn found 44 already cached array results,11 left to compute out of 55\n[stopes_launcher][INFO] - submitted job array for populate_index.OPQ64,IVF65536,PQ64.bn: ['48535900_0', ..., '48535900_10']\n[mining_utils][WARNING] - No mapping for lang hi\n[embed_text][INFO] - Number of shards: 55\n[embed_text][INFO] - Embed hi (hi), 55 files\n[stopes_launcher][INFO] - for encode.hi.55 found 55 already cached array results,0 left to compute out of 55\n[train_faiss_index][INFO] - lang=hi, sents=162844151, required=40000000, index type=OPQ64,IVF65536,PQ64\n")),(0,i.kt)("p",null,"We can see that the launcher has found out that it doesn't need to run the encode and train index steps for the bn lang (source language) and can skip straight to populating the index with embeddings, but it also already processed 44 shards for that step, so will only re-schedule jobs for 11 shards. In parallel, it is also processing the target language (hi) and found that it still needs to run the index training step as it also recovered all the encoded shards."),(0,i.kt)("p",null,"If you are using slurm as the launcher instead of the local setting, the pipeline also takes care of communicating with slurm, waiting for all slurm jobs to finish and synchronizing the consecutive jobs. See below on how to run single steps for debugging."),(0,i.kt)("p",null,"You can run the whole pipeline locally with:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python global_mining_pipeline.py src_lang=bn tgt_lang=hi +data=ccg launcher.cluster=local\n")),(0,i.kt)("h1",{id:"understanding-the-configuration"},"Understanding the Configuration"),(0,i.kt)("p",null,"The configuration is driven by ",(0,i.kt)("a",{parentName:"p",href:"https://hydra.cc/"},"Hydra"),", this makes it sound way more complicated than it actually is. The first main difference is how the command line arguments are specified. Instead of using the ",(0,i.kt)("inlineCode",{parentName:"p"},"--arg=foobar")," standard notation, Hydra introduces its ",(0,i.kt)("a",{parentName:"p",href:"https://hydra.cc/docs/1.0/advanced/override_grammar/basic/#basic-override-syntax"},"own notation")," to be able to have a  more complete syntax. This is indeed odd, but once you are used to it, it provides a lot of benefits."),(0,i.kt)("p",null,"A second big change is that most of the things that can be changed in the pipeline are driven by yaml configuration files instead of having to change the script files. These configuration files are checked in and you can override them on the command line (see the examples above). The pipeline will log the actual full config+overrides in the output folder when you do a run, so that you can always look at the config that was used to generate a particular data folder."),(0,i.kt)("p",null,'The third major change, and main benefit, is that the configs are split in "groups" (hydra terminology) and you can override a whole group with another yaml file with a very simple syntax. For instance, the embed_text step has a set of pre-made configs in ',(0,i.kt)("inlineCode",{parentName:"p"},"global_mining/conf/embed_text")," and you can swap between them. If you would like to make a new reusable/shared config for embed_text, you could put a new yaml file in that that folder (let say ",(0,i.kt)("inlineCode",{parentName:"p"},"global_mining/conf/embed_text/foobar.yaml"),") and select it from the cli with:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python global_mining_pipeline.py src_lang=bn tgt_lang=hi +data=ccg embed_text=foobar\n")),(0,i.kt)("p",null,"See the Data and Modules discussion below for more examples."),(0,i.kt)("h2",{id:"outputs-and-working-dir"},"Outputs and Working Dir"),(0,i.kt)("p",null,'The output of the pipeline is set in the global_mining.yaml to be ".", which means the current working directory. When running ',(0,i.kt)("inlineCode",{parentName:"p"},"global_mining_pipeline.py")," it will by default create a new folder under ",(0,i.kt)("inlineCode",{parentName:"p"},"outputs/today_date/timeofrun")," and make this your working directory. This means all your logs will be well organized. It also means that the main output of each step will go under that directory given the default configuration of ",(0,i.kt)("inlineCode",{parentName:"p"},"output_dir: .")),(0,i.kt)("p",null,'Because you might run the pipeline multiple times for the same "data run" (e.g. if it fails with pre-emption in the middle, etc.), this default config means that you might end up with data spread across multiple date/time directories.'),(0,i.kt)("p",null,"It's therefore a good idea when you are doing a full run (not just testing), to specify a fixed outputs directory when launching the pipeline:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python global_mining_pipeline.py src_lang=bn tgt_lang=hi +data=ccg output_dir=/myfinal/data/outputs\n")),(0,i.kt)("p",null,"This way logs and other temp files will go to the working directory, but the data will go to a clean central place."),(0,i.kt)("h2",{id:"data"},"Data"),(0,i.kt)("p",null,"The current data configuration for the pipeline takes a few parameters:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"data_version"),(0,i.kt)("li",{parentName:"ul"},"iteration"),(0,i.kt)("li",{parentName:"ul"},"data_shard_dir"),(0,i.kt)("li",{parentName:"ul"},"shard_type"),(0,i.kt)("li",{parentName:"ul"},"bname")),(0,i.kt)("p",null,'Because you will most often always use the same data for your runs, there is no need to specify this every time on the CLI or in the default config. There is a "group" under ',(0,i.kt)("inlineCode",{parentName:"p"},"global_mining/conf/data")," where you can put common data sources. Checkout the demo config to see how to configure data. You can create a data config folder if you want to switch data without changing all other presets."),(0,i.kt)("h1",{id:"modules"},"Modules"),(0,i.kt)("p",null,"The pipeline is made of seven main steps:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"embed_text"),(0,i.kt)("li",{parentName:"ul"},"train_index"),(0,i.kt)("li",{parentName:"ul"},"populate_index"),(0,i.kt)("li",{parentName:"ul"},"merge_index"),(0,i.kt)("li",{parentName:"ul"},"calculate_distances"),(0,i.kt)("li",{parentName:"ul"},"mine_indexes"),(0,i.kt)("li",{parentName:"ul"},"mine_sentences")),(0,i.kt)("p",null,'Each of them is configured as a "group" and their configurations can be overridden by switching groups on the cli as explained above. This override can also completely switch the code/module that is being used to compute this step, without changing the pipeline itself.'),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Embedding Modules")),(0,i.kt)("p",null,"You can switch the actual encoder being used to choose between multiple encoders. For example, you can choose to use LaBSE, BERT, RoBERTa, or any other model from the sentence-transformers repo within the HuggingFace Model Hub (",(0,i.kt)("a",{parentName:"p",href:"https://huggingface.co/sentence-transformers"},"https://huggingface.co/sentence-transformers"),"). Here\u2019s an example of how to encode text using LaBSE (with encoder-specific options in blue):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python global_mining_pipeline.py src_lang=bn tgt_lang=hi +data=ccg  embed_text=hf_roberta_large\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python global_mining_pipeline.py src_lang=bn tgt_lang=hi +data=ccg  embed_text=hf_labse\n")),(0,i.kt)("p",null,"or you can choose any huggingface encoder by their name with:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python global_mining_pipeline.py -c job src_lang=bn tgt_lang=hi +data=ccg  embed_text=huggingface embed_text.encoder_model=sentence-transformers/LaBSE\n")),(0,i.kt)("p",null,"These are shortcuts to common models, but you can switch to any other model in the HuggingFace Model Hub, see ",(0,i.kt)("inlineCode",{parentName:"p"},"hf_labse.yaml")," for an example of how to change config.encoder.encoder_model. To utilise LASER you can use the following example command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python global_mining_pipeline.py -c job src_lang=bn tgt_lang=hi +data=ccg embed_text=laser2\nembed_text.config.encoder.encoder_model=path_to_laser_model\nembed_text.config.encoder.spm_model=path_to_spm_model\n")),(0,i.kt)("h1",{id:"sweeping-multi-run"},"Sweeping (multi-run)"),(0,i.kt)("p",null,"One of the benefits of the hydra cli override syntax, is that you can ask hydra to try different variations of the configuration with a simple command line. Hydra calls this ",(0,i.kt)("a",{parentName:"p",href:"https://hydra.cc/docs/1.0/tutorials/basic/running_your_app/multi-run/"},'"multi-run"')," and it lets you specify variations to your config that you would like to try."),(0,i.kt)("p",null,"For instance, if you would like to run the pipeline on multiple languages, you can do:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python global_mining_pipeline.py -m src_lang=en tgt_lang=bn,hi +data=ccg\n")),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"-m")," parameter tells the pipeline to start with multi-run and ",(0,i.kt)("inlineCode",{parentName:"p"},"tgt_lang=bn,hi")," tells it to make two runs, one for en-bn and one for en-hi."),(0,i.kt)("p",null," You could also sweep over the lang and the encoders with:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python global_mining_pipeline.py -m src_lang=en tgt_lang=bn,hi +data=ccg embed_text=hf_roberta_large,hf_labse\n")),(0,i.kt)("h1",{id:"nmt-model-training-on-mined-bitexts"},"NMT model training on mined bitexts"),(0,i.kt)("p",null,"Once a mined bitext has been produced, ",(0,i.kt)("inlineCode",{parentName:"p"},"stopes")," can then run an end-to-end bilingual NMT system. It follows the following steps:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Takes as input a mined bitext (format: alignment-score ","[tab]"," text ","[tab]"," text)"),(0,i.kt)("li",{parentName:"ol"},"Applies threshold filters based on alignment score and max number of alignments to use for training"),(0,i.kt)("li",{parentName:"ol"},"Applies moses preprocessing (on bitext only)"),(0,i.kt)("li",{parentName:"ol"},"Trains spm (on bitext only)"),(0,i.kt)("li",{parentName:"ol"},"Spm-encodes bitext and chosen evaluation data"),(0,i.kt)("li",{parentName:"ol"},"Binarizes files for fairseq"),(0,i.kt)("li",{parentName:"ol"},"Trains bilingual NMT using ",(0,i.kt)("inlineCode",{parentName:"li"},"fairseq-train")," on binarized data"),(0,i.kt)("li",{parentName:"ol"},"Runs ",(0,i.kt)("inlineCode",{parentName:"li"},"fairseq-generate")," on binarized evaluation data for all model checkpoints"),(0,i.kt)("li",{parentName:"ol"},"Calculates BLEU scores")),(0,i.kt)("h1",{id:"run-it"},"Run it"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"python -m stopes.pipelines.bitext.nmt_bitext_eval                   \\\nsrc_lang=lin_Latn tgt_lang=eng_Latn                                 \\\ninput_file_mined_data_tsv=/path/to/your/bitext.tsv                  \\\npreproc_binarize_mined.test_data_dir.dataset_name=flores200         \\\npreproc_binarize_mined.test_data_dir.base_dir=/path/to/flores200    \\\noutput_dir=/directory/to/store/preprocessed/data/and/checkpoints    \\\nlauncher.cache.caching_dir=/path/to/cache                           \\\nmaximum_epoch=20\n")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"NOTE"),": In order for the training pipeline to know which column of the bitext corresponds to the selected ",(0,i.kt)("inlineCode",{parentName:"p"},"src_lang")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"tgt_lang"),", it presumes that the two text columns in the bitext are ordered by their sorted language names. For example, for a ",(0,i.kt)("inlineCode",{parentName:"p"},"eng-lin")," bitext, the format is: alignment-score ","[tab]"," english-text ","[tab]"," lingala-text (not alignment-score ","[tab]"," lingala-text ","[tab]"," english-text). "),(0,i.kt)("h2",{id:"outputs"},"Outputs"),(0,i.kt)("p",null,"The NMT pipeline will create the following directories in the specified ",(0,i.kt)("inlineCode",{parentName:"p"},"output_dir"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"bin_dir"),": moses preprocessed, spm-encoded, and binarized data."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"trained_models"),": checkpoints from ",(0,i.kt)("inlineCode",{parentName:"li"},"fairseq-train"),". ",(0,i.kt)("strong",{parentName:"li"},"Note"),": this directory will also contain files containing the outputs of both ",(0,i.kt)("inlineCode",{parentName:"li"},"fairseq-generate")," (files ending in ",(0,i.kt)("inlineCode",{parentName:"li"},".out"),") and the corresponding BLEU evaluations for each checkpoint (files ending in ",(0,i.kt)("inlineCode",{parentName:"li"},".bleu"),").")),(0,i.kt)("h2",{id:"evaluation-data"},"Evaluation data"),(0,i.kt)("p",null,"To find the evaluation data for your chosen languages, ",(0,i.kt)("inlineCode",{parentName:"p"},"stopes")," needs to know the relevant path. See ",(0,i.kt)("inlineCode",{parentName:"p"},"path")," in ",(0,i.kt)("inlineCode",{parentName:"p"},"stopes/pipelines/bitext/conf/preproc_binarize_mined/standard_conf.yaml"),". Currently it defaults to the format of the ",(0,i.kt)("inlineCode",{parentName:"p"},"flores200")," dataset. To use this, please ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/flores/tree/main/flores200"},"download flores200"),". "),(0,i.kt)("h2",{id:"example-overrides"},"Example overrides"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Spm training")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"spm.train.config.vocab_size=7000")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Model configuation")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"train_fairseq.config.params.optimization.lr=[0.0001]\ntrain_fairseq.config.params.optimization.update_freq=[8]\ntrain_fairseq.config.params.model.encoder_layers=6\ntrain_fairseq.config.params.model.encoder_embed_dim=512\ntrain_fairseq.config.params.model.dropout=0.3\ntrain_fairseq.config.num_gpus=8\n")))}u.isMDXComponent=!0}}]);
# @package _global_

# configure the launcher, this one will
# decide how each step of the pipeline gets executed
launcher:
  # `local` means that you will run all the steps sequentially on
  # your local computer. You can also use `slurm` if you have a slurm cluster
  # setup, in which case paralell jobs will be submitted when possible.
  cluster: local
  # we don't need to set this if we aren't using slurm
  partition: null
  # To improve resilience and make iteration faster, stopes caches the results of
  # each steps of the pipeline. Set a fixed directory here if you want to
  # leverage caching.
  cache:
    caching_dir: /tmp/global_mining_cache

# you will need to set this on the CLI to point to where
# the demo dir is (after running demo/mining/prepare.sh)
demo_dir: ???

# where will the data go, `.` is the current run directory (auto generated by
# hydra to be unique for each run)
output_dir: .
# where to find models and vocab, this is what `prepare.sh` downloaded
model_dir: ${demo_dir}/models/wmt22
vocab_dir: ${demo_dir}/models/wmt22

# Setup some of the steps, using GPU for populate_index makes
# it a lot faster, but if you don't have one, it's ok.
populate_index:
  config:
    use_gpu: False

embedding_sample:
  sample_shards: False

train_index:
  config:
    use_gpu: False

calculate_distances:
  config:
    gpu_memory_gb: 32
    gpu_type: ""  # don't use gpu

# Provides info about the data. A lot of this is used to generate nice output
# file names.
data:
  data_version: V32m
  iteration: 1
  data_shard_dir: ${demo_dir}
  shard_type: text
  bname: demo_wmt22
  # shard_glob tells us where to find the language files `{lang}` will be
  # replaced by the language code from src and tgt
  shard_glob: ${.data_shard_dir}/{lang}.gz
  # we need to know the number of lines in each file, this is computed in
  # prepare.sh and this tells the pipeline where to find the files with this
  # info
  nl_file_template: "{lang}.nl"

# for each language we support, specify where the laser encoder is and where the
# spm model/vocab can be found. In our case, we have custom laser2/3 encoders
# for all languages. But for most languages we reuse the same spm/vocab, so we
# use hydra to share this value.
default_spm: ${model_dir}/laser2.spm
default_vocab: ${model_dir}/laser2.cvocab
lang_configs:
  amh:
    encoder_model: ${model_dir}/laser3-amh.v1.pt
    spm_model: ${model_dir}/laser3-amh.v1.spm
    spm_vocab: ${model_dir}/laser3-amh.v1.cvocab
  fuv:
    encoder_model: ${model_dir}/laser3-fuv.v1.pt
    spm_model: ${model_dir}/laser3-fuv.v1.spm
    spm_vocab: ${model_dir}/laser3-fuv.v1.cvocab
  hau:
    encoder_model: ${model_dir}/laser3-hau.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  ibo:
    encoder_model: ${model_dir}/laser3-ibo.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  kam:
    encoder_model: ${model_dir}/laser3-kam.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  lin:
    encoder_model: ${model_dir}/laser3-lin.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  lug:
    encoder_model: ${model_dir}/laser3-lug.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  luo:
    encoder_model: ${model_dir}/laser3-luo.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  nso:
    encoder_model: ${model_dir}/laser3-nso.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  nya:
    encoder_model: ${model_dir}/laser3-nya.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  orm:
    encoder_model: ${model_dir}/laser3-orm.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  sna:
    encoder_model: ${model_dir}/laser3-sna.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  ssw:
    encoder_model: ${model_dir}/laser3-ssw.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  swh:
    encoder_model: ${model_dir}/laser3-swh.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  tsn:
    encoder_model: ${model_dir}/laser3-tsn.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  tso:
    encoder_model: ${model_dir}/laser3-tso.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  umb:
    encoder_model: ${model_dir}/laser3-umb.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  wol:
    encoder_model: ${model_dir}/laser3-wol.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  xho:
    encoder_model: ${model_dir}/laser3-xho.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  yor:
    encoder_model: ${model_dir}/laser3-yor.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
  zul:
    encoder_model: ${model_dir}/laser3-zul.v1.pt
    spm_model: ${default_spm}
    spm_vocab: ${default_vocab}
